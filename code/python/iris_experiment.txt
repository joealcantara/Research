import numpy as np
import pandas as pd
from itertools import combinations, product
import time
from sklearn.datasets import load_iris

# Load Iris
iris = load_iris()
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
df_iris['species'] = iris.target

print(f"Dataset shape: {df_iris.shape}")
print(f"\nFirst few rows:")
print(df_iris.head())
print(f"\nSpecies distribution:")
print(df_iris['species'].value_counts())

# Discretize to binary variables
# Strategy: above/below median for continuous, binary grouping for species

df_binary = pd.DataFrame()

# Continuous features: 1 if above median, 0 otherwise
for col in ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']:
    median = df_iris[col].median()
    short_name = col.split()[0]  # 'sepal' or 'petal'
    dimension = col.split()[1]   # 'length' or 'width'
    df_binary[f'{short_name}_{dimension}'] = (df_iris[col] > median).astype(int)

# Species: setosa (0) vs non-setosa (1,2)
df_binary['is_not_setosa'] = (df_iris['species'] > 0).astype(int)

print("Discretized dataset:")
print(df_binary.head(10))
print(f"\nShape: {df_binary.shape}")
print(f"\nVariable names: {list(df_binary.columns)}")

def count_edges(structure):
    """Count total edges in a structure."""
    return sum(len(parents) for parents in structure.values())


def is_dag(edges, n):
    """
    Check if a set of directed edges forms a DAG (no cycles).
    Uses DFS to detect cycles.
    """
    # Build adjacency list
    adj = {i: [] for i in range(n)}
    for i, j in edges:
        adj[i].append(j)
    
    # DFS with visited and recursion stack
    visited = [False] * n
    rec_stack = [False] * n
    
    def has_cycle(node):
        visited[node] = True
        rec_stack[node] = True
        
        for neighbor in adj[node]:
            if not visited[neighbor]:
                if has_cycle(neighbor):
                    return True
            elif rec_stack[neighbor]:
                # Back edge found - cycle detected
                return True
        
        rec_stack[node] = False
        return False
    
    # Check each component
    for i in range(n):
        if not visited[i]:
            if has_cycle(i):
                return False
    
    return True


def generate_all_dags(variables):
    """
    Generate all possible DAG structures for a given set of variables.
    
    For n variables, we consider all possible directed edges between them.
    For each subset of edges, we check if it forms a DAG (acyclic).
    
    For 5 variables: 20 possible directed edges, 2^20 = 1,048,576 subsets to check.
    Results in 29,281 valid DAGs.
    """
    n = len(variables)
    
    # All possible directed edges (i, j) where i != j
    all_edges = [(i, j) for i in range(n) for j in range(n) if i != j]
    
    print(f"Generating DAGs for {n} variables...")
    print(f"Possible edges: {len(all_edges)}")
    print(f"Edge subsets to check: {2**len(all_edges):,}")
    
    # Generate all subsets of edges
    dags = []
    start_time = time.time()
    
    for num_edges in range(len(all_edges) + 1):
        for edge_subset in combinations(all_edges, num_edges):
            # Check if this edge set forms a DAG (acyclic)
            if is_dag(edge_subset, n):
                # Convert to structure format: {variable: [list of parents]}
                structure = {}
                for i, var in enumerate(variables):
                    # Find parents of this variable (edges that point TO this variable)
                    parents = [variables[j] for j, k in edge_subset if k == i]
                    structure[var] = parents
                
                dags.append(structure)
    
    elapsed = time.time() - start_time
    print(f"Generated {len(dags):,} DAGs in {elapsed:.2f} seconds")
    
    return dags

variables = list(df_binary.columns)
print(f"Variables: {variables}")
print()

all_dags = generate_all_dags(variables)

# Edge distribution
edge_counts = {}
for dag in all_dags:
    edges = count_edges(dag)
    edge_counts[edges] = edge_counts.get(edges, 0) + 1

print(f"\nEdge distribution:")
for edges in sorted(edge_counts.keys()):
    print(f"  {edges} edges: {edge_counts[edges]:,} DAGs")

def estimate_parameters(df, structure):
    """
    Estimate conditional probability tables from data.
    """
    params = {}
    
    for var, parents in structure.items():
        params[var] = {}
        
        if len(parents) == 0:
            # No parents: just marginal probability
            count_1 = (df[var] == 1).sum()
            total = len(df)
            # Add smoothing to avoid 0 probabilities
            params[var][()] = (count_1 + 1) / (total + 2)
        
        else:
            # Has parents: compute conditional probability for each parent combination
            all_combos = list(product([0, 1], repeat=len(parents)))
            
            for combo in all_combos:
                # Filter data where parents match this combo
                mask = pd.Series([True] * len(df))
                for i, parent in enumerate(parents):
                    mask = mask & (df[parent] == combo[i])
                
                subset = df[mask]
                count_1 = (subset[var] == 1).sum()
                total = len(subset)
                
                # Add smoothing
                params[var][combo] = (count_1 + 1) / (total + 2)
    
    return params


def compute_log_likelihood(df, structure, params):
    """
    Compute log probability of the data given the structure and parameters.
    """
    log_lik = 0.0
    
    for idx, row in df.iterrows():
        for var, parents in structure.items():
            # Get parent values for this row
            if len(parents) == 0:
                parent_combo = ()
            else:
                parent_combo = tuple(row[p] for p in parents)
            
            # Get probability of this variable's value
            p_1 = params[var][parent_combo]
            
            if row[var] == 1:
                log_lik += np.log(p_1)
            else:
                log_lik += np.log(1 - p_1)
    
    return log_lik


def compute_posteriors(df, structures, lambda_penalty=2.0):
    """
    Compute posterior probability for each structure.
    """
    print(f"Computing posteriors for {len(structures):,} structures...")
    start_time = time.time()
    
    results = []
    
    for i, structure in enumerate(structures):
        if (i + 1) % 5000 == 0:
            print(f"  Progress: {i+1:,} / {len(structures):,}")
        
        params = estimate_parameters(df, structure)
        log_lik = compute_log_likelihood(df, structure, params)
        edges = count_edges(structure)
        
        log_prior = -lambda_penalty * edges
        log_posterior = log_lik + log_prior
        
        results.append({
            'structure': structure,
            'log_likelihood': log_lik,
            'edges': edges,
            'log_prior': log_prior,
            'log_posterior': log_posterior
        })
    
    # Normalize to get probabilities
    log_posteriors = np.array([r['log_posterior'] for r in results])
    log_posteriors_shifted = log_posteriors - log_posteriors.max()
    posteriors = np.exp(log_posteriors_shifted)
    posteriors = posteriors / posteriors.sum()
    
    for i in range(len(results)):
        results[i]['posterior'] = posteriors[i]
    
    elapsed = time.time() - start_time
    print(f"Completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)")
    
    return results

results = compute_posteriors(df_binary, all_dags, lambda_penalty=2.0)

# Sort by posterior probability
sorted_results = sorted(results, key=lambda x: -x['posterior'])

print("Top 20 structures by posterior probability:")
print(f"{'Rank':4} | {'Edges':5} | {'Log-lik':>10} | {'Log-post':>10} | {'Posterior':>10} | Structure")
print("-" * 100)

for rank, r in enumerate(sorted_results[:20], 1):
    # Create readable structure string
    edges_list = []
    for var, parents in r['structure'].items():
        for parent in parents:
            edges_list.append(f"{parent}→{var}")
    
    if len(edges_list) == 0:
        structure_str = "(independent)"
    else:
        structure_str = ", ".join(edges_list[:3])  # Show first 3 edges
        if len(edges_list) > 3:
            structure_str += f" (+{len(edges_list)-3} more)"
    
    print(f"{rank:4} | {r['edges']:5} | {r['log_likelihood']:10.2f} | {r['log_posterior']:10.2f} | {r['posterior']:10.6f} | {structure_str}")

# Summary statistics
print(f"\nPosterior concentration:")
print(f"  Top 1:  {sorted_results[0]['posterior']:.6f}")
print(f"  Top 5:  {sum(r['posterior'] for r in sorted_results[:5]):.6f}")
print(f"  Top 10: {sum(r['posterior'] for r in sorted_results[:10]):.6f}")
print(f"  Top 20: {sum(r['posterior'] for r in sorted_results[:20]):.6f}")
print(f"  Top 50: {sum(r['posterior'] for r in sorted_results[:50]):.6f}")

# Look at the top structure in detail
print("=== MAP (Maximum A Posteriori) Structure ===")
print()
top_structure = sorted_results[0]['structure']

for var in variables:
    parents = top_structure[var]
    if len(parents) == 0:
        print(f"{var}: (no parents - root node)")
    else:
        print(f"{var}: ← {', '.join(parents)}")

print(f"\nTotal edges: {count_edges(top_structure)}")
print(f"Posterior probability: {sorted_results[0]['posterior']:.6f}")

from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import log_loss, mean_squared_error

# Prepare continuous data with column name mapping
df_continuous = df_iris.copy()
df_continuous.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']

print("Continuous dataset:")
print(df_continuous.head())
print(f"\nShape: {df_continuous.shape}")
print(f"\nVariable types:")
for col in df_continuous.columns:
    if col == 'species':
        print(f"  {col}: categorical (0=setosa, 1=versicolor, 2=virginica)")
    else:
        print(f"  {col}: continuous")

def estimate_parameters_tree(df, structure, max_depth=2):
    """
    Fit decision trees for each variable given its parents.
    
    Returns a dictionary of fitted trees:
    - DecisionTreeClassifier for categorical variables (species)
    - DecisionTreeRegressor for continuous variables
    """
    models = {}
    
    for var, parents in structure.items():
        if len(parents) == 0:
            # No parents: store marginal statistics
            if var == 'species':
                # Categorical: store class probabilities
                models[var] = {
                    'type': 'categorical_marginal',
                    'probs': df[var].value_counts(normalize=True).sort_index().values
                }
            else:
                # Continuous: store mean and std
                models[var] = {
                    'type': 'continuous_marginal',
                    'mean': df[var].mean(),
                    'std': df[var].std()
                }
        else:
            # Has parents: fit decision tree
            X = df[parents].values
            y = df[var].values
            
            if var == 'species':
                # Categorical variable
                tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
                tree.fit(X, y)
                models[var] = {
                    'type': 'categorical_tree',
                    'tree': tree,
                    'parents': parents
                }
            else:
                # Continuous variable
                tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)
                tree.fit(X, y)
                # Compute residual std for likelihood calculation
                predictions = tree.predict(X)
                residuals = y - predictions
                models[var] = {
                    'type': 'continuous_tree',
                    'tree': tree,
                    'parents': parents,
                    'residual_std': np.std(residuals) + 1e-6  # Add small constant to avoid zero
                }
    
    return models

def compute_log_likelihood_tree(df, structure, models):
    """
    Compute log probability of data using fitted decision trees.
    """
    log_lik = 0.0
    
    for idx, row in df.iterrows():
        for var in structure.keys():
            model = models[var]
            
            if model['type'] == 'categorical_marginal':
                # Marginal categorical: P(species = k)
                class_idx = int(row[var])
                prob = model['probs'][class_idx]
                log_lik += np.log(prob + 1e-10)  # Add small constant to avoid log(0)
                
            elif model['type'] == 'continuous_marginal':
                # Marginal continuous: Gaussian likelihood
                value = row[var]
                mean = model['mean']
                std = model['std']
                log_lik += -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((value - mean) / std)**2
                
            elif model['type'] == 'categorical_tree':
                # Conditional categorical: P(species = k | parents)
                X = row[model['parents']].values.reshape(1, -1)
                probs = model['tree'].predict_proba(X)[0]
                class_idx = int(row[var])
                prob = probs[class_idx]
                log_lik += np.log(prob + 1e-10)
                
            elif model['type'] == 'continuous_tree':
                # Conditional continuous: Gaussian around tree prediction
                X = row[model['parents']].values.reshape(1, -1)
                prediction = model['tree'].predict(X)[0]
                value = row[var]
                std = model['residual_std']
                log_lik += -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((value - prediction) / std)**2
    
    return log_lik

def compute_posteriors_tree(df, structures, lambda_penalty=2.0, max_depth=2):
    """
    Compute posterior probability for each structure using decision trees.
    """
    print(f"Computing posteriors with decision trees for {len(structures):,} structures...")
    print(f"  max_depth={max_depth}, lambda={lambda_penalty}")
    start_time = time.time()
    
    results = []
    
    for i, structure in enumerate(structures):
        if (i + 1) % 5000 == 0:
            elapsed = time.time() - start_time
            print(f"  Progress: {i+1:,} / {len(structures):,} ({elapsed:.1f}s elapsed)")
        
        models = estimate_parameters_tree(df, structure, max_depth=max_depth)
        log_lik = compute_log_likelihood_tree(df, structure, models)
        edges = count_edges(structure)
        
        log_prior = -lambda_penalty * edges
        log_posterior = log_lik + log_prior
        
        results.append({
            'structure': structure,
            'log_likelihood': log_lik,
            'edges': edges,
            'log_prior': log_prior,
            'log_posterior': log_posterior,
            'models': models
        })
    
    # Normalize to get probabilities
    log_posteriors = np.array([r['log_posterior'] for r in results])
    log_posteriors_shifted = log_posteriors - log_posteriors.max()
    posteriors = np.exp(log_posteriors_shifted)
    posteriors = posteriors / posteriors.sum()
    
    for i in range(len(results)):
        results[i]['posterior'] = posteriors[i]
    
    elapsed = time.time() - start_time
    print(f"Completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)")
    
    return results

variables_continuous = list(df_continuous.columns)
print(f"Variables: {variables_continuous}")
print()

# Generate all DAGs (reuse same DAGs, just different variable names)
all_dags_continuous = generate_all_dags(variables_continuous)

results_tree = compute_posteriors_tree(df_continuous, all_dags_continuous, lambda_penalty=2.0, max_depth=2)

# Sort by posterior probability
sorted_results_tree = sorted(results_tree, key=lambda x: -x['posterior'])

print("Top 20 structures by posterior probability (Decision Trees):")
print(f"{'Rank':4} | {'Edges':5} | {'Log-lik':>10} | {'Log-post':>10} | {'Posterior':>10} | Structure")
print("-" * 100)

for rank, r in enumerate(sorted_results_tree[:20], 1):
    # Create readable structure string
    edges_list = []
    for var, parents in r['structure'].items():
        for parent in parents:
            edges_list.append(f"{parent}→{var}")
    
    if len(edges_list) == 0:
        structure_str = "(independent)"
    else:
        structure_str = ", ".join(edges_list[:3])  # Show first 3 edges
        if len(edges_list) > 3:
            structure_str += f" (+{len(edges_list)-3} more)"
    
    print(f"{rank:4} | {r['edges']:5} | {r['log_likelihood']:10.2f} | {r['log_posterior']:10.2f} | {r['posterior']:10.6f} | {structure_str}")

# Summary statistics
print(f"\nPosterior concentration:")
print(f"  Top 1:   {sorted_results_tree[0]['posterior']:.6f}")
print(f"  Top 5:   {sum(r['posterior'] for r in sorted_results_tree[:5]):.6f}")
print(f"  Top 10:  {sum(r['posterior'] for r in sorted_results_tree[:10]):.6f}")
print(f"  Top 20:  {sum(r['posterior'] for r in sorted_results_tree[:20]):.6f}")
print(f"  Top 50:  {sum(r['posterior'] for r in sorted_results_tree[:50]):.6f}")
print(f"  Top 100: {sum(r['posterior'] for r in sorted_results_tree[:100]):.6f}")

print("=" * 80)
print("COMPARISON: Binary Discretization vs Decision Trees")
print("=" * 80)
print()
print(f"{'Metric':<30} | {'Binary':>12} | {'Trees':>12} | {'Improvement':>12}")
print("-" * 80)
print(f"{'Top 1 posterior':<30} | {sorted_results[0]['posterior']:>12.6f} | {sorted_results_tree[0]['posterior']:>12.6f} | {sorted_results_tree[0]['posterior'] / sorted_results[0]['posterior']:>12.1f}x")
print(f"{'Top 10 cumulative':<30} | {sum(r['posterior'] for r in sorted_results[:10]):>12.6f} | {sum(r['posterior'] for r in sorted_results_tree[:10]):>12.6f} | {sum(r['posterior'] for r in sorted_results_tree[:10]) / sum(r['posterior'] for r in sorted_results[:10]):>12.1f}x")
print(f"{'Top 100 cumulative':<30} | {sum(r['posterior'] for r in sorted_results[:100]):>12.6f} | {sum(r['posterior'] for r in sorted_results_tree[:100]):>12.6f} | {sum(r['posterior'] for r in sorted_results_tree[:100]) / sum(r['posterior'] for r in sorted_results[:100]):>12.1f}x")
print()
print("KEY INSIGHT:")
print("  Decision trees achieve MUCH better posterior concentration because they can")
print("  express threshold-based relationships (IF-THEN rules) that linear models cannot.")
print()
print("  Example: 'IF petal_length < 2.5 THEN species = setosa'")
print("  - Decision tree: Natural to express")
print("  - Linear model: Fundamentally wrong tool")
print()
print(f"  The {sorted_results_tree[0]['posterior'] / sorted_results[0]['posterior']:.1f}x improvement in top posterior demonstrates the")
print("  importance of using the RIGHT THEORY LANGUAGE for the problem.")

print("=" * 80)
print("MAP (Maximum A Posteriori) Structure - Decision Tree Approach")
print("=" * 80)
print()

top_structure_tree = sorted_results_tree[0]['structure']
top_models = sorted_results_tree[0]['models']

for var in variables_continuous:
    parents = top_structure_tree[var]
    model = top_models[var]
    
    if len(parents) == 0:
        print(f"{var}:")
        print(f"  No parents (root node)")
        if model['type'] == 'categorical_marginal':
            print(f"  Marginal probabilities: {model['probs']}")
        else:
            print(f"  Mean: {model['mean']:.3f}, Std: {model['std']:.3f}")
    else:
        print(f"{var} ← {', '.join(parents)}:")
        if model['type'] == 'categorical_tree':
            print(f"  Decision tree classifier (max_depth=2)")
        else:
            print(f"  Decision tree regressor (max_depth=2)")
            print(f"  Residual std: {model['residual_std']:.3f}")
    print()

print(f"Total edges: {count_edges(top_structure_tree)}")
print(f"Posterior probability: {sorted_results_tree[0]['posterior']:.6f}")
print()
print("This structure captures the causal relationships in Iris using interpretable")
print("IF-THEN rules rather than linear approximations.")
